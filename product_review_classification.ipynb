{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 4 Code Challenge: Product Reviews\n",
    "\n",
    "This assessment is designed to test your understanding of these areas:\n",
    "\n",
    "1. Data Engineering\n",
    "    - Understanding an existing ETL pipeline\n",
    "    - Feature scaling\n",
    "2. Deep Learning with Neural Networks\n",
    "    - Creating a TensorFlow neural network model\n",
    "    - Fitting the model on training data\n",
    "    - Hyperparameter tuning\n",
    "    - Model evaluation on test data\n",
    "3. Business Understanding and Technical Communication\n",
    "    - Advising a business on what kind of model architecture to use\n",
    "\n",
    "**Unlike previous challenges, we have provided you some pre-existing code.**  Your work, markdown and code, should build off of the pre-existing material. \n",
    "\n",
    "Make sure that your code is clean and readable, and that each step of your process is documented. For this challenge each step builds upon the step before it. If you are having issues finishing one of the steps completely, move on to the next step to attempt every section.  There will be occasional hints to help you move on to the next step if you get stuck, but attempt to follow the requirements whenever possible. \n",
    "\n",
    "### Business Understanding\n",
    "\n",
    "Northwind Trading Company allows customers to leave reviews, but those reviews do not have customer-facing \"star ratings\".  Instead, customers are free to write text, and other customers can vote on whether the review was helpful.  They find that this is a good trade-off between helping customers make informed decisions about products, and avoiding having any products go unsold because of poor ratings.\n",
    "\n",
    "Internally, Northwind is interested to know which of these reviews are positive, and which are negative.  **A previous employee of the company has already built a Random Forest Classifier model to perform this classification task.**\n",
    "\n",
    "Northwind management has heard great things about using Artificial Intelligence for this kind of task, especially Neural Networks like TensorFlow.  **You have been instructed to build a TensorFlow model and advise the company on whether they should switch from the Random Forest Classifier to the TensorFlow model.**\n",
    "\n",
    "In either case, you want a **classification model** that optimizes for **accuracy**.\n",
    "\n",
    "### Data Understanding\n",
    "\n",
    "The data has already been described, imported, and preprocessed in this notebook.\n",
    "\n",
    "****Below is the work of a previous employee. Take a brief moment to review their work and then complete the tasks at the bottom of the notebook.****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Review Classification\n",
    "\n",
    "## Business Understanding\n",
    "Our company wants a tool that will automatically classify product reviews as _positive_ or _negative_ reviews, based on the features of the review.  This will help our Product team to perform more sophisticated analyses in the future to help ensure customer satisfaction.\n",
    "\n",
    "## Data Understanding\n",
    "We have a labeled collection of 20,000 product reviews, with an equal split of positive and negative reviews. The dataset contains the following features:\n",
    "\n",
    " - `ProductId` Unique identifier for the product\n",
    " - `UserId` Unqiue identifier for the user\n",
    " - `ProfileName` Profile name of the user\n",
    " - `HelpfulnessNumerator` Number of users who found the review helpful\n",
    " - `HelpfulnessDenominator` Number of users who indicated whether they found the review helpful or not\n",
    " - `Time` Timestamp for the review\n",
    " - `Summary` Brief summary of the review\n",
    " - `Text` Text of the review\n",
    " - `PositiveReview` 1 if this was labeled as a positive review, 0 if it was labeled as a negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>PositiveReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B002QWHJOU</td>\n",
       "      <td>A37565LZHTG1VH</td>\n",
       "      <td>C. Maltese</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1305331200</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>This is a great product. My 2 year old Golden ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000ESLJ6C</td>\n",
       "      <td>AMUAWXDJHE4D2</td>\n",
       "      <td>angieseashore</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1320710400</td>\n",
       "      <td>Was there a recipe change?</td>\n",
       "      <td>I have been drinking Pero ever since I was a l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B004IJJQK4</td>\n",
       "      <td>AMHHNAFJ9L958</td>\n",
       "      <td>A M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1321747200</td>\n",
       "      <td>These taste so bland.</td>\n",
       "      <td>Look, each pack contains two servings of 120 c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId    ProfileName  HelpfulnessNumerator  \\\n",
       "0  B002QWHJOU  A37565LZHTG1VH     C. Maltese                     1   \n",
       "1  B000ESLJ6C   AMUAWXDJHE4D2  angieseashore                     1   \n",
       "2  B004IJJQK4   AMHHNAFJ9L958            A M                     0   \n",
       "\n",
       "   HelpfulnessDenominator        Time                     Summary  \\\n",
       "0                       1  1305331200                    Awesome!   \n",
       "1                       1  1320710400  Was there a recipe change?   \n",
       "2                       1  1321747200       These taste so bland.   \n",
       "\n",
       "                                                Text  PositiveReview  \n",
       "0  This is a great product. My 2 year old Golden ...               1  \n",
       "1  I have been drinking Pero ever since I was a l...               0  \n",
       "2  Look, each pack contains two servings of 120 c...               0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"reviews.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has already been cleaned, so there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               0\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Time                      0\n",
       "Summary                   0\n",
       "Text                      0\n",
       "PositiveReview            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PositiveReview` is the target, and all other columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"PositiveReview\", axis=1)\n",
    "y = df[\"PositiveReview\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 8)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, prepare for modeling. The following `Pipeline` prepares all data for modeling.  It one-hot encodes the `ProductId`, applies a tf-idf vectorizer to the `Summary` and `Text`, keeps the numeric columns as-is, and drops all other columns.\n",
    "\n",
    "The following code may take up to 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 11275)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_irrelevant_columns(X):\n",
    "    return X.drop([\"UserId\", \"ProfileName\"], axis=1)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"drop_columns\", FunctionTransformer(drop_irrelevant_columns)),\n",
    "    (\"transform_text_columns\", ColumnTransformer(transformers=[\n",
    "        (\"ohe\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\", sparse=False), [\"ProductId\"]),\n",
    "        (\"summary-tf-idf\", TfidfVectorizer(max_features=1000), \"Summary\"),\n",
    "        (\"text-tf-idf\", TfidfVectorizer(max_features=1000), \"Text\")\n",
    "    ], remainder=\"passthrough\"))\n",
    "])\n",
    "\n",
    "X_train_transformed = pipeline.fit_transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gotta scale it or else you spend 15 minutes wondering why your nn is performing horibbly. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_final = ss.fit_transform(X_train_transformed)\n",
    "X_test_final = ss.transform(X_test_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Fit a `RandomForestClassifier` with the best hyperparameters.  The following code may take up to 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=30, min_samples_split=15, random_state=42)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=30,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "rfc.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We are using _accuracy_ as our metric, which is the default metric in Scikit-Learn, so it is possible to just use the built-in `.score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9846666666666667\n",
      "Test accuracy: 0.9116\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", rfc.score(X_train_transformed, y_train))\n",
    "print(\"Test accuracy:\", rfc.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion matrix:\n",
      "[[7323  166]\n",
      " [  64 7447]]\n",
      "Test confusion matrix:\n",
      "[[2286  225]\n",
      " [ 217 2272]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train confusion matrix:\")\n",
    "print(confusion_matrix(y_train, rfc.predict(X_train_transformed)))\n",
    "print(\"Test confusion matrix:\")\n",
    "print(confusion_matrix(y_test, rfc.predict(X_test_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Interpretation\n",
    "\n",
    "The tuned Random Forest Classifier model appears to be somewhat overfit on the training data, but nevertheless achieves 91% accuracy on the test data.  Of the 9% of mislabeled comments, about half are false positives and half are false negatives.\n",
    "\n",
    "Because this is a balanced dataset, 91% accuracy is a substantial improvement over a 50% baseline.  This model is ready for production use for decision support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5279 - acc: 0.7396\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1740 - acc: 0.9547\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0823 - acc: 0.9815\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0502 - acc: 0.9889\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0348 - acc: 0.9927\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0262 - acc: 0.9948\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0209 - acc: 0.9962\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0175 - acc: 0.9965\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0151 - acc: 0.9969\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0134 - acc: 0.9972\n"
     ]
    }
   ],
   "source": [
    "# first model \n",
    "model = Sequential() \n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_final.shape[1],)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'SGD',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['acc'])\n",
    "\n",
    "baseline_model = model.fit(X_train_final, \n",
    "                    y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7203 - acc: 0.5469\n",
      "Epoch 2/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6442 - acc: 0.7158\n",
      "Epoch 3/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4865 - acc: 0.8323\n",
      "Epoch 4/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3185 - acc: 0.8907\n",
      "Epoch 5/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.2153 - acc: 0.9314\n",
      "Epoch 6/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1663 - acc: 0.9502\n",
      "Epoch 7/15\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1333 - acc: 0.9582\n",
      "Epoch 8/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1108 - acc: 0.9644\n",
      "Epoch 9/15\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.0929 - acc: 0.9693\n",
      "Epoch 10/15\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0867 - acc: 0.9697\n",
      "Epoch 11/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0848 - acc: 0.9702\n",
      "Epoch 12/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0821 - acc: 0.9704\n",
      "Epoch 13/15\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0753 - acc: 0.9731\n",
      "Epoch 14/15\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0741 - acc: 0.9717\n",
      "Epoch 15/15\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0691 - acc: 0.9745\n"
     ]
    }
   ],
   "source": [
    "# gonna add some dropout layers and l2 so its less likely to be overfit\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(10, activation='relu', input_shape=(X_train_final.shape[1],)))\n",
    "model3.add(Dense(4, activation = 'tanh'))\n",
    "model3.add(Dropout(0.3)) # dropout layer\n",
    "model3.add(Dense(4, activation = 'relu'))\n",
    "model3.add(Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu')) #l2 layer\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dropout(0.3)) # dropout layer\n",
    "model3.add(Dense(40, activation = 'relu'))\n",
    "model3.add(Dense(1, activation ='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['acc'])\n",
    "\n",
    "third_model =  model3.fit(X_train_final, \n",
    "                                y_train, \n",
    "                                epochs=15, \n",
    "                                batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prolly go with number model 3 as its less overfit \n",
    "# lets run it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2239,  272],\n",
       "       [ 210, 2279]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_test = model3.predict_classes(X_test_final);\n",
    "confusion_matrix(y_test, y_hat_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = third_model.history['acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9744666814804077\n",
      "Test accuracy: 0.9036\n"
     ]
    }
   ],
   "source": [
    "# NN\n",
    "print(f'Train accuracy: {Accuracy}')\n",
    "print(f'Test accuracy: {accuracy_score(y_test, y_hat_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9846666666666667\n",
      "Test accuracy: 0.9116\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "print(\"Train accuracy:\", rfc.score(X_train_transformed, y_train))\n",
    "print(\"Test accuracy:\", rfc.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Id stick with the random forest model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just followed the procedure they did, but you could have just broke out into a validation set to make sure you werent overfitting or do cross val. data set was pretty big so i didnt bother. first model was obviously overfit as you were almost getting 100% accuracy.I added some drop layers and regularization to prevent overfitting. got a lower accuracy but ensured it would likely perform better on holdout set. \n",
    "\n",
    "Id keep the random forest of 91% accuracy even if a NN beat it by a little bit. A lot more interprabilty and you can act on what features are going to drive the negative reviews down. Especially for a company that probably wants to undertstand whats going on it would make sense to use a random forest over a NN regardless of results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
